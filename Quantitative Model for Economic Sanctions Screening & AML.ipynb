{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e264e5a9-3a50-420b-995b-f6465d1cc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz # For fuzzy string matching\n",
    "from fuzzywuzzy import process # For more advanced fuzzy matching (though not directly used in this version, good to have)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # For splitting data and hyperparameter tuning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "import joblib # For saving/loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d516d99f-fa18-4ce6-ac73-ac70503f8d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.1-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.27.1-cp311-cp311-win_amd64.whl (100 kB)\n",
      "   ---------------------------------------- 0.0/100.4 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 61.4/100.4 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 100.4/100.4 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.13.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.6 MB 2.3 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.1/1.6 MB 1.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/1.6 MB 1.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.4/1.6 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.6 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.6/1.6 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.0/1.6 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.1/1.6 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.3/1.6 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.6 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7b11e4-ef85-4f93-a94b-049dc327a3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8562b59-763e-476d-bf80-5ae53dbb8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration and Global Variables ---\n",
    "NUM_CUSTOMERS = 500 # Number of dummy customer records\n",
    "NUM_TRANSACTIONS = 5000000 # Number of dummy transaction records (not directly used in this script, but from previous step)\n",
    "\n",
    "# Configuration for loading common names from Excel\n",
    "COMMON_NAMES_FILE = 'Name_list.xlsx' # Your Excel file for common names\n",
    "COMMON_NAMES_SHEET = 'Names' # Your sheet name within the Excel file\n",
    "\n",
    "# List of common full names (will be loaded from Excel or use default if file not found)\n",
    "COMMON_FULL_NAMES = [] # Will be populated by load_common_names_from_excel\n",
    "\n",
    "# Country Risk Map (simulated external data)\n",
    "COUNTRY_RISK_MAP = {\n",
    "    'IRAN': 'HIGH', 'NORTH KOREA': 'HIGH', 'SYRIA': 'HIGH', 'CUBA': 'HIGH', 'VENEZUULA': 'HIGH',\n",
    "    'RUSSIA': 'MEDIUM', 'CHINA': 'MEDIUM', 'INDIA': 'LOW', 'USA': 'LOW', 'UK': 'LOW',\n",
    "    'GERMANY': 'LOW', 'FRANCE': 'LOW', 'BRAZIL': 'MEDIUM', 'SOUTH AFRICA': 'MEDIUM',\n",
    "    'NIGERIA': 'MEDIUM', 'AFGHANISTAN': 'HIGH', 'YEMEN': 'HIGH', 'SOMALIA': 'HIGH',\n",
    "    'LEBANON': 'MEDIUM', 'PAKISTAN': 'MEDIUM'\n",
    "}\n",
    "HIGH_RISK_COUNTRIES = [country for country, risk in COUNTRY_RISK_MAP.items() if risk == 'HIGH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6c39d54-13ed-4b39-bb3c-aa9da82d0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper Functions for Data Loading and Generation ---\n",
    "\n",
    "def load_common_names_from_excel(filepath, sheet_name):\n",
    "    \"\"\"\n",
    "    Loads a list of full names from an Excel file.\n",
    "    Assumes the names are in a column named 'Sanctioned_name' in the specified sheet.\n",
    "    Provides a fallback to a hardcoded list if the file or column is not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        names_df = pd.read_excel(filepath, sheet_name=sheet_name)\n",
    "        if 'Sanctioned_name' in names_df.columns:\n",
    "            print(f\"Successfully loaded common names from '{filepath}' sheet '{sheet_name}'.\")\n",
    "            return names_df['Sanctioned_name'].astype(str).tolist()\n",
    "        else:\n",
    "            print(f\"Error: 'Sanctioned_name' column not found in '{sheet_name}' of '{filepath}'.\")\n",
    "            return _get_default_common_names()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Common names file '{filepath}' not found. Using default hardcoded names.\")\n",
    "        return _get_default_common_names()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading common names from Excel: {e}\")\n",
    "        return _get_default_common_names()\n",
    "\n",
    "def _get_default_common_names():\n",
    "    \"\"\"Provides a hardcoded list of common names as a fallback.\"\"\"\n",
    "    return [\n",
    "        'John Smith', 'Jane Johnson', 'Michael Williams', 'Emily Brown', 'David Jones',\n",
    "        'Sarah Garcia', 'Chris Miller', 'Anna Davis', 'Robert Rodriguez', 'Maria Martinez',\n",
    "        'William Taylor', 'Olivia Wilson', 'James Moore', 'Sophia White', 'Benjamin Green',\n",
    "        'Isabella Hall', 'Lucas King', 'Mia Wright', 'Henry Lopez', 'Charlotte Hill'\n",
    "    ]\n",
    "\n",
    "def load_or_generate_initial_data(sanctions_csv_path='UK Sanctions List_mean.csv',\n",
    "                                   customer_data_path='customer_data.csv',\n",
    "                                   num_customers=NUM_CUSTOMERS):\n",
    "    \"\"\"\n",
    "    Loads cleaned sanctions and customer data. If files are not found,\n",
    "    it generates minimal dummy data for demonstration.\n",
    "    \"\"\"\n",
    "    global COMMON_FULL_NAMES # Declare global to modify the list\n",
    "    COMMON_FULL_NAMES = load_common_names_from_excel(COMMON_NAMES_FILE, COMMON_NAMES_SHEET)\n",
    "    if not COMMON_FULL_NAMES:\n",
    "        print(\"FATAL: COMMON_FULL_NAMES list is empty. Cannot proceed with data generation.\")\n",
    "        return pd.DataFrame(), pd.DataFrame() # Return empty DFs\n",
    "\n",
    "    sanctions_df_cleaned = pd.DataFrame()\n",
    "    customer_df = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Load sanctions data\n",
    "        # Assuming 'header=1' for UK Sanctions List_mean.csv based on previous context\n",
    "        raw_sanctions_df = pd.read_csv(sanctions_csv_path, encoding='utf-8', header=1)\n",
    "        print(f\"Successfully loaded {sanctions_csv_path}. Shape: {raw_sanctions_df.shape}\")\n",
    "\n",
    "        # Clean and standardize sanctions data\n",
    "        name_col = 'Name 6'\n",
    "        address_col = 'Address 6'\n",
    "        dob_col = 'DOB 6'\n",
    "        nationality_col = 'Nationality 6'\n",
    "        type_col = 'Type'\n",
    "        id_col = 'ID'\n",
    "\n",
    "        # Robust column handling\n",
    "        if name_col not in raw_sanctions_df.columns:\n",
    "            print(f\"Warning: '{name_col}' not found in sanctions CSV. Please check column names.\")\n",
    "            # Fallback to a generic 'Name' if exists, or create a placeholder\n",
    "            name_col = 'Name' if 'Name' in raw_sanctions_df.columns else raw_sanctions_df.columns[0] # Take first col as fallback\n",
    "            print(f\"Using '{name_col}' as name column fallback.\")\n",
    "        # Similar checks for other columns if necessary\n",
    "\n",
    "        sanctions_df_cleaned = raw_sanctions_df.copy()\n",
    "        sanctions_df_cleaned['Sanctioned_Name'] = sanctions_df_cleaned[name_col].astype(str).str.upper().str.strip()\n",
    "        sanctions_df_cleaned['Sanctioned_Address'] = sanctions_df_cleaned[address_col].astype(str).str.upper().str.strip()\n",
    "        sanctions_df_cleaned['Sanctioned_DOB'] = pd.to_datetime(sanctions_df_cleaned[dob_col], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "        sanctions_df_cleaned['Sanctioned_Nationality'] = sanctions_df_cleaned[nationality_col].astype(str).str.upper().str.strip()\n",
    "        sanctions_df_cleaned['Sanction_Type'] = sanctions_df_cleaned[type_col].astype(str).str.upper().str.strip()\n",
    "        sanctions_df_cleaned['Sanctioned_ID'] = sanctions_df_cleaned[id_col].astype(str) if id_col in sanctions_df_cleaned.columns else [f'S{i:04d}' for i in range(len(sanctions_df_cleaned))]\n",
    "\n",
    "        sanctions_df_cleaned = sanctions_df_cleaned[[\n",
    "            'Sanctioned_ID', 'Sanctioned_Name', 'Sanctioned_Address',\n",
    "            'Sanctioned_DOB', 'Sanctioned_Nationality', 'Sanction_Type'\n",
    "        ]].copy()\n",
    "        sanctions_df_cleaned = sanctions_df_cleaned[\n",
    "            (sanctions_df_cleaned['Sanctioned_Name'] != 'UNKNOWN SANCTIONED NAME') &\n",
    "            (sanctions_df_cleaned['Sanctioned_Name'] != 'NAN') &\n",
    "            (sanctions_df_cleaned['Sanctioned_Name'].str.strip() != '')\n",
    "        ].reset_index(drop=True)\n",
    "        print(\"Cleaned Sanctions Data (first 3 rows):\")\n",
    "        print(sanctions_df_cleaned.head(3))\n",
    "        sanctions_df_cleaned.to_csv('sanctions_list_cleaned.csv', index=False) # Save cleaned version\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Sanctions file '{sanctions_csv_path}' not found. Generating minimal dummy sanctions data.\")\n",
    "        sanctions_data_fallback = {\n",
    "            'Sanctioned_ID': [f'S{i:04d}' for i in range(1, 11)],\n",
    "            'Sanctioned_Name': ['JOHN DOE', 'JANE SMITH', 'ALI BABA', 'VLADIMIR PUTIN', 'KIM JONG-UN',\n",
    "                                'MOHAMMAD AL-SHAMSI', 'FATIMA ZAHRA', 'GLOBAL OIL CORP', 'SEA DRAGON VESSEL', 'NORTH STAR BANK'],\n",
    "            'Sanctioned_Address': ['123 MAIN ST, TEHRAN, IRAN', '456 OAK AVE, DAMASCUS, SYRIA', '789 DESERT RD, BAGHDAD, IRAQ',\n",
    "                                   'KREMLIN, MOSCOW, RUSSIA', 'PYONGYANG, NORTH KOREA', 'RIYADH, SAUDI ARABIA',\n",
    "                                   'BEIRUT, LEBANON', 'SHANGHAI, CHINA', 'PORT OF BANDAR ABBAS, IRAN', 'MOSCOW, RUSSIA'],\n",
    "            'Sanctioned_DOB': ['1970-01-15', '1985-03-20', '1960-11-01', '1952-10-07', '1984-01-08',\n",
    "                               '1975-07-22', '1990-09-10', np.nan, np.nan, np.nan],\n",
    "            'Sanctioned_Nationality': ['IRANIAN', 'SYRIAN', 'IRAQI', 'RUSSIAN', 'NORTH KOREAN',\n",
    "                                       'SAUDI', 'LEBANESE', np.nan, np.nan, np.nan],\n",
    "            'Sanction_Type': ['INDIVIDUAL', 'INDIVIDUAL', 'ENTITY', 'INDIVIDUAL', 'INDIVIDUAL',\n",
    "                              'INDIVIDUAL', 'INDIVIDUAL', 'ENTITY', 'VESSEL', 'ENTITY']\n",
    "        }\n",
    "        sanctions_df_cleaned = pd.DataFrame(sanctions_data_fallback)\n",
    "        print(\"Generated fallback sanctions data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during sanctions data loading/cleaning: {e}\")\n",
    "        sanctions_df_cleaned = pd.DataFrame()\n",
    "\n",
    "\n",
    "    try:\n",
    "        customer_df = pd.read_csv(customer_data_path)\n",
    "        print(f\"Successfully loaded customer data from {customer_data_path}. Shape: {customer_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Customer data file '{customer_data_path}' not found. Generating dummy customer data.\")\n",
    "        customers = []\n",
    "        for i in range(1, num_customers + 1):\n",
    "            customer_id = f'CUST{i:05d}'\n",
    "            customer_name = random.choice(COMMON_FULL_NAMES)\n",
    "            customer_address = f\"{random.randint(100, 999)} {random.choice(['Main St', 'Oak Ave', 'Pine Ln'])}\"\n",
    "            customer_dob = (datetime.date(1950, 1, 1) + datetime.timedelta(days=random.randint(0, 365 * 50))).strftime('%Y-%m-%d')\n",
    "            customer_nationality = random.choice(list(COUNTRY_RISK_MAP.keys()))\n",
    "            customer_country = random.choice(list(COUNTRY_RISK_MAP.keys()))\n",
    "            customer_industry = random.choice(['Financial Services', 'Retail', 'Technology', 'Manufacturing', 'Healthcare'])\n",
    "            onboarding_date = (datetime.date(2020, 1, 1) + datetime.timedelta(days=random.randint(0, 365 * 3))).strftime('%Y-%m-%d')\n",
    "\n",
    "            # Introduce some 'risky' customers that might match sanctions list\n",
    "            if i % 10 == 0 and not sanctions_df_cleaned.empty:\n",
    "                sanctioned_entity = sanctions_df_cleaned.sample(1).iloc[0]\n",
    "                customer_name = sanctioned_entity['Sanctioned_Name'].replace('A', 'a', 1).replace('E', 'e', 1) # Slight variation\n",
    "                customer_address = sanctioned_entity['Sanctioned_Address'].replace('ST', 'Street', 1) # Slight variation\n",
    "                customer_dob = sanctioned_entity['Sanctioned_DOB'] # Exact DOB match\n",
    "                customer_nationality = sanctioned_entity['Sanctioned_Nationality']\n",
    "                customer_country = sanctioned_entity['Sanctioned_Nationality']\n",
    "\n",
    "            customers.append({\n",
    "                'Customer_ID': customer_id,\n",
    "                'Customer_Name': customer_name,\n",
    "                'Customer_Address': customer_address,\n",
    "                'Customer_DOB': customer_dob,\n",
    "                'Customer_Nationality': customer_nationality,\n",
    "                'Customer_Country': customer_country,\n",
    "                'Customer_Industry': customer_industry,\n",
    "                'Onboarding_Date': onboarding_date\n",
    "            })\n",
    "        customer_df = pd.DataFrame(customers)\n",
    "        customer_df.to_csv(customer_data_path, index=False) # Save generated customer data\n",
    "        print(\"Generated dummy customer data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during customer data loading/generation: {e}\")\n",
    "        customer_df = pd.DataFrame()\n",
    "\n",
    "    return sanctions_df_cleaned, customer_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b5a4b8a-5211-4fc0-b786-4a7ae5584e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Feature Engineering Function ---\n",
    "def calculate_sanctions_features(df):\n",
    "    \"\"\"\n",
    "    Calculates various matching and risk features for each customer-sanctioned entity pair.\n",
    "    Uses more robust string matching.\n",
    "    \"\"\"\n",
    "    # Ensure columns are string type and clean them\n",
    "    df['Customer_Name_Clean'] = df['Customer_Name'].astype(str).str.upper().str.strip()\n",
    "    df['Sanctioned_Name_Clean'] = df['Sanctioned_Name'].astype(str).str.upper().str.strip()\n",
    "    df['Customer_Address_Clean'] = df['Customer_Address'].astype(str).str.upper().str.strip()\n",
    "    df['Sanctioned_Address_Clean'] = df['Sanctioned_Address'].astype(str).str.upper().str.strip()\n",
    "    df['Customer_Nationality_Clean'] = df['Customer_Nationality'].astype(str).str.upper().str.strip()\n",
    "    df['Sanctioned_Nationality_Clean'] = df['Sanctioned_Nationality'].astype(str).str.upper().str.strip()\n",
    "    df['Customer_Country_Clean'] = df['Customer_Country'].astype(str).str.upper().str.strip()\n",
    "\n",
    "    # Name Fuzzy Match Scores\n",
    "    # fuzz.ratio: Simple Levenshtein ratio\n",
    "    df['name_fuzz_ratio'] = df.apply(\n",
    "        lambda row: fuzz.ratio(row['Customer_Name_Clean'], row['Sanctioned_Name_Clean']), axis=1\n",
    "    )\n",
    "    # fuzz.token_sort_ratio: Sorts tokens before matching, good for reordered names\n",
    "    df['name_token_sort_ratio'] = df.apply(\n",
    "        lambda row: fuzz.token_sort_ratio(row['Customer_Name_Clean'], row['Sanctioned_Name_Clean']), axis=1\n",
    "    )\n",
    "    # fuzz.token_set_ratio: Considers common tokens, robust to extra words\n",
    "    df['name_token_set_ratio'] = df.apply(\n",
    "        lambda row: fuzz.token_set_ratio(row['Customer_Name_Clean'], row['Sanctioned_Name_Clean']), axis=1\n",
    "    )\n",
    "    # Take the max of these for a comprehensive name score\n",
    "    df['name_match_score'] = df[['name_fuzz_ratio', 'name_token_sort_ratio', 'name_token_set_ratio']].max(axis=1)\n",
    "\n",
    "\n",
    "    # Address Fuzzy Match Score (using token_set_ratio for robustness to word order)\n",
    "    df['address_match_score'] = df.apply(\n",
    "        lambda row: fuzz.token_set_ratio(row['Customer_Address_Clean'], row['Sanctioned_Address_Clean']), axis=1\n",
    "    )\n",
    "\n",
    "    # Date of Birth Match (Exact match)\n",
    "    df['dob_match'] = df.apply(\n",
    "        lambda row: 1 if (pd.notna(row['Customer_DOB']) and pd.notna(row['Sanctioned_DOB']) and\n",
    "                         str(row['Customer_DOB']) == str(row['Sanctioned_DOB'])) else 0, axis=1\n",
    "    )\n",
    "    \n",
    "    # Nationality Match (Exact match)\n",
    "    df['nationality_match'] = df.apply(\n",
    "        lambda row: 1 if (pd.notna(row['Customer_Nationality_Clean']) and pd.notna(row['Sanctioned_Nationality_Clean']) and\n",
    "                         row['Customer_Nationality_Clean'] == row['Sanctioned_Nationality_Clean']) else 0, axis=1\n",
    "    )\n",
    "\n",
    "    # Country Risk Factor for Customer's Country\n",
    "    df['customer_country_risk_score'] = df['Customer_Country_Clean'].map(\n",
    "        {k: (10 if v == 'HIGH' else 5 if v == 'MEDIUM' else 1) for k, v in COUNTRY_RISK_MAP.items()}\n",
    "    ).fillna(0) # Assign 0 if country not in map\n",
    "\n",
    "    # Sanction Type Encoding (Numerical severity)\n",
    "    sanction_type_severity = {\n",
    "        'INDIVIDUAL': 10,\n",
    "        'ENTITY': 8,\n",
    "        'VESSEL': 5,\n",
    "        'AIRCRAFT': 5,\n",
    "        'NAN': 0,\n",
    "        'UNKNOWN': 0 # Handle any other missing/unknown types\n",
    "    }\n",
    "    df['sanction_type_severity_score'] = df['Sanction_Type'].map(sanction_type_severity).fillna(0)\n",
    "\n",
    "    # Interaction features (example)\n",
    "    df['name_country_interaction'] = df['name_match_score'] * df['customer_country_risk_score']\n",
    "    df['name_dob_interaction'] = df['name_match_score'] * df['dob_match']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a3b22db-f019-47fd-951d-ab27071adec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Training Data Generation and Labeling ---\n",
    "num_samples=20000\n",
    "num_true_positives = int(num_samples * 0.05)\n",
    "def generate_and_label_training_data(customer_df, sanctions_df_cleaned, num_samples):\n",
    "    \"\"\"\n",
    "    Generates a simulated labeled dataset for training the sanctions screening model.\n",
    "    It creates a mix of true positives and true negatives based on predefined rules.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Simulated Training Data with {num_samples} samples ---\")\n",
    "    \n",
    "    training_samples = []\n",
    "    \n",
    "    # Strategy 1: Generate True Positives (simulated matches)\n",
    "    num_true_positives = int(num_samples * 0.05) # Aim for 5% true positives\n",
    "    for _ in range(num_true_positives):\n",
    "        if sanctions_df_cleaned.empty or customer_df.empty:\n",
    "            break\n",
    "        sanctioned_entity = sanctions_df_cleaned.sample(1).iloc[0]\n",
    "        customer_entity = customer_df.sample(1).iloc[0]\n",
    "\n",
    "        # Create a \"true match\" by making customer data very similar to sanctioned\n",
    "        # Introduce slight variations to simulate fuzzy matches\n",
    "        cust_name_tp = sanctioned_entity['Sanctioned_Name']\n",
    "        if len(cust_name_tp) > 3: # Introduce a typo for fuzzy match\n",
    "            idx = random.randint(0, len(cust_name_tp) - 1)\n",
    "            cust_name_tp = cust_name_tp[:idx] + random.choice(string.ascii_uppercase) + cust_name_tp[idx+1:]\n",
    "        \n",
    "        cust_address_tp = sanctioned_entity['Sanctioned_Address']\n",
    "        if len(cust_address_tp) > 5: # Introduce a slight address variation\n",
    "            idx = random.randint(0, len(cust_address_tp) - 1)\n",
    "            cust_address_tp = cust_address_tp[:idx] + random.choice(string.ascii_uppercase) + cust_address_tp[idx+1:]\n",
    "\n",
    "        cust_dob_tp = sanctioned_entity['Sanctioned_DOB']\n",
    "        cust_nationality_tp = sanctioned_entity['Sanctioned_Nationality']\n",
    "        cust_country_tp = sanctioned_entity['Sanctioned_Nationality'] # Assume country is same as nationality for TP\n",
    "\n",
    "        training_samples.append({\n",
    "            'Customer_ID': customer_entity['Customer_ID'],\n",
    "            'Customer_Name': cust_name_tp,\n",
    "            'Customer_Address': cust_address_tp,\n",
    "            'Customer_DOB': cust_dob_tp,\n",
    "            'Customer_Nationality': cust_nationality_tp,\n",
    "            'Customer_Country': cust_country_tp,\n",
    "            'Sanctioned_ID': sanctioned_entity['Sanctioned_ID'],\n",
    "            'Sanctioned_Name': sanctioned_entity['Sanctioned_Name'],\n",
    "            'Sanctioned_Address': sanctioned_entity['Sanctioned_Address'],\n",
    "            'Sanctioned_DOB': sanctioned_entity['Sanctioned_DOB'],\n",
    "            'Sanctioned_Nationality': sanctioned_entity['Sanctioned_Nationality'],\n",
    "            'Sanction_Type': sanctioned_entity['Sanction_Type'],\n",
    "            'is_sanction_match': 1 # Label as True Positive\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a608e82-e220-48c3-9f9b-e14627cd0608",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sanctions_df_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_true_negatives \u001b[38;5;241m=\u001b[39m num_samples \u001b[38;5;241m-\u001b[39m num_true_positives\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_true_negatives):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sanctions_df_cleaned\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;129;01mor\u001b[39;00m customer_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     sanctioned_entity \u001b[38;5;241m=\u001b[39m sanctions_df_cleaned\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sanctions_df_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Strategy 2: Generate True Negatives (simulated non-matches)\n",
    "    num_true_negatives = num_samples - num_true_positives\n",
    "    for _ in range(num_true_negatives):\n",
    "        if sanctions_df_cleaned.empty or customer_df.empty:\n",
    "            break\n",
    "        sanctioned_entity = sanctions_df_cleaned.sample(1).iloc[0]\n",
    "        customer_entity = customer_df.sample(1).iloc[0]\n",
    "\n",
    "        # Ensure it's a \"true negative\" by picking very different entities\n",
    "        # Or by ensuring names/addresses are very different\n",
    "        cust_name_tn = random.choice(COMMON_FULL_NAMES)\n",
    "        cust_address_tn = f\"{random.randint(1000, 9999)} {random.choice(['Road', 'Lane', 'Square'])}\"\n",
    "        cust_dob_tn = (datetime.date(1940, 1, 1) + datetime.timedelta(days=random.randint(0, 365 * 60))).strftime('%Y-%m-%d')\n",
    "        cust_nationality_tn = random.choice(LOW_RISK_COUNTRIES)\n",
    "        cust_country_tn = random.choice(LOW_RISK_COUNTRIES)\n",
    "\n",
    "        training_samples.append({\n",
    "            'Customer_ID': customer_entity['Customer_ID'],\n",
    "            'Customer_Name': cust_name_tn,\n",
    "            'Customer_Address': cust_address_tn,\n",
    "            'Customer_DOB': cust_dob_tn,\n",
    "            'Customer_Nationality': cust_nationality_tn,\n",
    "            'Customer_Country': cust_country_tn,\n",
    "            'Sanctioned_ID': sanctioned_entity['Sanctioned_ID'],\n",
    "            'Sanctioned_Name': sanctioned_entity['Sanctioned_Name'],\n",
    "            'Sanctioned_Address': sanctioned_entity['Sanctioned_Address'],\n",
    "            'Sanctioned_DOB': sanctioned_entity['Sanctioned_DOB'],\n",
    "            'Sanctioned_Nationality': sanctioned_entity['Sanctioned_Nationality'],\n",
    "            'Sanction_Type': sanctioned_entity['Sanction_Type'],\n",
    "            'is_sanction_match': 0 # Label as True Negative\n",
    "        })\n",
    "    \n",
    "    training_df = pd.DataFrame(training_samples)\n",
    "    training_df_features = calculate_sanctions_features(training_df.copy())\n",
    "    \n",
    "    # Fill any remaining NaNs in features with 0 (or a more sophisticated imputation)\n",
    "    feature_cols = [\n",
    "        'name_match_score', 'address_match_score', 'dob_match',\n",
    "        'nationality_match', 'customer_country_risk_score', 'sanction_type_severity_score',\n",
    "        'name_country_interaction', 'name_dob_interaction'\n",
    "    ]\n",
    "    training_df_features[feature_cols] = training_df_features[feature_cols].fillna(0)\n",
    "\n",
    "    print(f\"Generated {len(training_df_features)} training samples.\")\n",
    "    print(f\"Class distribution:\\n{training_df_features['is_sanction_match'].value_counts(normalize=True)}\")\n",
    "    print(\"Sample of training data features and labels:\")\n",
    "    print(training_df_features[['Customer_ID', 'Sanctioned_ID', 'name_match_score', 'dob_match', 'is_sanction_match']].head())\n",
    "    \n",
    "    return training_df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2613ce86-f37e-4cf1-85a3-46c1968cad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Model Training Function ---\n",
    "def train_sanctions_model(X_train, y_train, model_type='GradientBoosting'):\n",
    "    \"\"\"\n",
    "    Trains a sanctions screening classification model.\n",
    "    Supports Logistic Regression and Gradient Boosting.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training {model_type} Model ---\")\n",
    "    if model_type == 'LogisticRegression':\n",
    "        model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', max_iter=1000)\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "    elif model_type == 'GradientBoosting':\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'LogisticRegression' or 'GradientBoosting'\")\n",
    "\n",
    "    # Use GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters for {model_type}: {grid_search.best_params_}\")\n",
    "    print(f\"Best ROC AUC score for {model_type}: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model\n",
    "\n",
    "# --- Model Evaluation Function ---\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluates the trained model and plots ROC and Precision-Recall curves.\"\"\"\n",
    "    print(f\"\\n--- Evaluation for {model_name} ---\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"ROC AUC Score for {model_name}: {roc_auc:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    auprc = auc(recall, precision)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'{model_name} (AUPRC = {auprc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve for {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Feature Importance (if applicable)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"\\nFeature Importance:\")\n",
    "        feature_importance = pd.Series(model.feature_importances_, index=X_test.columns).sort_values(ascending=False)\n",
    "        print(feature_importance)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=feature_importance.values, y=feature_importance.index)\n",
    "        plt.title(f'Feature Importance for {model_name}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4af4d8fd-b2dd-4503-8a87-89ad0daa8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Sanctions Screening Function ---\n",
    "def perform_sanctions_screening(customers_df, sanctions_df, model, features, alert_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Performs sanctions screening on customer data using the trained ML model.\n",
    "    Implements a basic blocking strategy to reduce the number of pairs for feature calculation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Performing Sanctions Screening on {len(customers_df)} Customers ---\")\n",
    "    \n",
    "    all_screening_results = []\n",
    "\n",
    "    # Apply a simple blocking strategy: only compare if first letter of last name matches\n",
    "    # This is a very basic blocking; real systems use more sophisticated methods (e.g., phonetic keys, n-grams)\n",
    "    customers_df['Customer_First_Letter'] = customers_df['Customer_Name'].astype(str).str.upper().str[0]\n",
    "    sanctions_df['Sanctioned_First_Letter'] = sanctions_df['Sanctioned_Name'].astype(str).str.upper().str[0]\n",
    "\n",
    "    unique_first_letters = customers_df['Customer_First_Letter'].unique()\n",
    "\n",
    "    for letter in unique_first_letters:\n",
    "        customer_block = customers_df[customers_df['Customer_First_Letter'] == letter].copy()\n",
    "        sanction_block = sanctions_df[sanctions_df['Sanctioned_First_Letter'] == letter].copy()\n",
    "\n",
    "        if sanction_block.empty:\n",
    "            continue # No sanctioned entities starting with this letter\n",
    "\n",
    "        block_pairs = []\n",
    "        for cust_idx, customer in customer_block.iterrows():\n",
    "            for sanc_idx, sanctioned in sanction_block.iterrows():\n",
    "                block_pairs.append({\n",
    "                    'Customer_ID': customer['Customer_ID'],\n",
    "                    'Customer_Name': customer['Customer_Name'],\n",
    "                    'Customer_Address': customer['Customer_Address'],\n",
    "                    'Customer_DOB': customer['Customer_DOB'],\n",
    "                    'Customer_Nationality': customer['Customer_Nationality'],\n",
    "                    'Customer_Country': customer['Customer_Country'],\n",
    "                    'Sanctioned_ID': sanctioned['Sanctioned_ID'],\n",
    "                    'Sanctioned_Name': sanctioned['Sanctioned_Name'],\n",
    "                    'Sanctioned_Address': sanctioned['Sanctioned_Address'],\n",
    "                    'Sanctioned_DOB': sanctioned['Sanctioned_DOB'],\n",
    "                    'Sanctioned_Nationality': sanctioned['Sanctioned_Nationality'],\n",
    "                    'Sanction_Type': sanctioned['Sanction_Type']\n",
    "                })\n",
    "        \n",
    "        if not block_pairs:\n",
    "            continue\n",
    "\n",
    "        screening_df_block = pd.DataFrame(block_pairs)\n",
    "        \n",
    "        # Calculate features for this block\n",
    "        screening_df_block_features = calculate_sanctions_features(screening_df_block.copy())\n",
    "        \n",
    "        # Select only the features used for training and fill NaNs\n",
    "        X_screen_block = screening_df_block_features[features].fillna(0)\n",
    "        \n",
    "        # Predict probabilities\n",
    "        screening_df_block_features['Sanction_Match_Probability'] = model.predict_proba(X_screen_block)[:, 1]\n",
    "        all_screening_results.append(screening_df_block_features)\n",
    "\n",
    "    if not all_screening_results:\n",
    "        print(\"No potential matches found after blocking strategy. All customers are OK.\")\n",
    "        # Create an empty DataFrame with expected columns if no results\n",
    "        return pd.DataFrame(columns=['Customer_ID', 'Customer_Name', 'Max_Sanction_Match_Probability', 'Top_Matched_Sanctioned_Entity', 'Sanction_Alert_Flag'])\n",
    "\n",
    "    full_screening_df = pd.concat(all_screening_results, ignore_index=True)\n",
    "\n",
    "    # Aggregate results per customer: take the maximum probability found across all sanctioned entities\n",
    "    # and identify the top matched sanctioned entity name\n",
    "    final_screening_results = full_screening_df.groupby('Customer_ID').agg(\n",
    "        Customer_Name=('Customer_Name', 'first'),\n",
    "        Max_Sanction_Match_Probability=('Sanction_Match_Probability', 'max'),\n",
    "        Top_Matched_Sanctioned_Entity=('Sanctioned_Name', lambda x: x.iloc[x.index[np.argmax(full_screening_df.loc[x.index, 'Sanction_Match_Probability'])]])\n",
    "    ).reset_index()\n",
    "\n",
    "    final_screening_results['Sanction_Alert_Flag'] = np.where(\n",
    "        final_screening_results['Max_Sanction_Match_Probability'] >= alert_threshold,\n",
    "        'ALERT', 'OK'\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Final Sanctions Screening Results Summary ---\")\n",
    "    print(f\"Total ALERTS: {final_screening_results[final_screening_results['Sanction_Alert_Flag'] == 'ALERT'].shape[0]}\")\n",
    "    print(f\"Total OK: {final_screening_results[final_screening_results['Sanction_Alert_Flag'] == 'OK'].shape[0]}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 Sanction Alerts (by probability):\")\n",
    "    print(final_screening_results[final_screening_results['Sanction_Alert_Flag'] == 'ALERT']\n",
    "          .sort_values(by='Max_Sanction_Match_Probability', ascending=False).head(10))\n",
    "    \n",
    "    print(f\"\\nSample of OK results (first 5):\")\n",
    "    print(final_screening_results[final_screening_results['Sanction_Alert_Flag'] == 'OK'].head(5))\n",
    "\n",
    "    return final_screening_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "170b069c-a949-4435-b350-6e97cf49f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Sanctions Screening ML Model Implementation ---\n",
      "Successfully loaded common names from 'Name_list.xlsx' sheet 'Names'.\n",
      "An unexpected error occurred during sanctions data loading/cleaning: 'utf-8' codec can't decode byte 0x99 in position 12: invalid start byte\n",
      "Successfully loaded customer data from customer_data.csv. Shape: (12820, 8)\n",
      "Initial data loading/generation failed. Exiting.\n",
      "\n",
      "--- Generating Simulated Training Data with 50000 samples ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Define features for the ML model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m ml_features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_match_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_match_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdob_match\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnationality_match\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_country_risk_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msanction_type_severity_score\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_country_interaction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_dob_interaction\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     21\u001b[0m ]\n\u001b[1;32m---> 23\u001b[0m X \u001b[38;5;241m=\u001b[39m training_data_features_labels[ml_features]\n\u001b[0;32m     24\u001b[0m y \u001b[38;5;241m=\u001b[39m training_data_features_labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_sanction_match\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Execution Flow ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Sanctions Screening ML Model Implementation ---\")\n",
    "\n",
    "    # 1. Load or Generate Initial Data\n",
    "    sanctions_df, customer_df = load_or_generate_initial_data()\n",
    "\n",
    "    if sanctions_df.empty or customer_df.empty:\n",
    "        print(\"Initial data loading/generation failed. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Generate Labeled Training Data\n",
    "    # Adjust num_samples based on your computational resources and desired training data size\n",
    "    training_data_features_labels = generate_and_label_training_data(customer_df, sanctions_df, num_samples=50000)\n",
    "\n",
    "    # Define features for the ML model\n",
    "    ml_features = [\n",
    "        'name_match_score', 'address_match_score', 'dob_match',\n",
    "        'nationality_match', 'customer_country_risk_score', 'sanction_type_severity_score',\n",
    "        'name_country_interaction', 'name_dob_interaction'\n",
    "    ]\n",
    "\n",
    "    X = training_data_features_labels[ml_features]\n",
    "    y = training_data_features_labels['is_sanction_match']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"\\nTraining data split: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "    print(f\"Testing data split: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "    # 3. Train the Sanctions Screening Model\n",
    "    # You can choose 'LogisticRegression' or 'GradientBoosting'\n",
    "    sanctions_model = train_sanctions_model(X_train, y_train, model_type='GradientBoosting')\n",
    "\n",
    "    # Save the trained model\n",
    "    model_filename = 'sanctions_screening_gb_model.joblib'\n",
    "    joblib.dump(sanctions_model, model_filename)\n",
    "    print(f\"Trained model saved to '{model_filename}'\")\n",
    "\n",
    "    # 4. Evaluate the Model\n",
    "    evaluate_model(sanctions_model, X_test, y_test, model_name=\"Gradient Boosting Sanctions Model\")\n",
    "\n",
    "    # 5. Perform Sanctions Screening on Customer Data\n",
    "    # Load the trained model if needed (e.g., in a production environment)\n",
    "    # loaded_model = joblib.load(model_filename)\n",
    "    \n",
    "    final_screening_results_df = perform_sanctions_screening(\n",
    "        customer_df, sanctions_df, sanctions_model, ml_features, alert_threshold=0.5\n",
    "    )\n",
    "\n",
    "    # You can now analyze final_screening_results_df\n",
    "    # For example, save it to CSV:\n",
    "    final_screening_results_df.to_csv('final_sanctions_screening_results.csv', index=False)\n",
    "    print(\"\\nFinal screening results saved to 'final_sanctions_screening_results.csv'\")\n",
    "\n",
    "    print(\"\\n--- Sanctions Screening ML Model Implementation Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7e89c-1761-4831-abe9-31d891756e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114aeb5-5cd2-40a8-ab7c-fc96436d2809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685fe57-19b3-48fb-be76-3c7872b94c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0ca92-febe-4925-ad75-d2a21d5e57d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b086a-88a7-49a2-8dff-1c779f845bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e9dc5-3d7e-499f-986a-8bc30bc7f2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077f7ed-4e45-48c3-be65-85dd7bdb1f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd5e92-2940-424d-9463-01992b6f5d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f25f2-eece-448c-8236-61dc1160af2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2b0b3-d408-44b4-8c72-b92d137d8b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a5be7-6f00-484e-995e-b4670d26e139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25d440-a11b-42e6-805a-67dc7a020c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da56cde-ea8a-4b32-819d-2db68fd17f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9711184-f413-4c01-833d-bf43370bdc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd91524-62e1-49f7-824d-005e5befe851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b29b50-c37b-4c26-b4f5-8b7af0707ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7775cf2-0066-4fde-9fd6-5e2c6d1ee0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
