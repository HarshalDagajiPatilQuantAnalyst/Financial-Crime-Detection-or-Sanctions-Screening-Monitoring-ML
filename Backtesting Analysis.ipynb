{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6569f6e-2ac9-47ed-a8a3-63d5634b16e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Backtesting Analysis ---\n",
      "Loaded Sanctions Screening Model from 'sanctions_screening_gb_model.joblib'.\n",
      "Loaded AML Isolation Forest Model and Scaler from 'aml_isolation_forest_model.joblib' and 'aml_scaler.joblib'.\n",
      "\n",
      "--- Running Sanctions Model Backtest ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp952\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hp952\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hp952\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Historical Sanctions Model Performance:\n",
      "    Accuracy: 0.9500\n",
      "    Precision (Sanctioned): 0.0000\n",
      "    Recall (Sanctioned): 0.0000\n",
      "    F1-Score (Sanctioned): 0.0000\n",
      "    ROC AUC: 1.0000\n",
      "\n",
      "--- Running AML Model Backtest ---\n",
      "  Historical AML Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.99      1.00      0.99     19795\n",
      "  Suspicious       0.50      0.31      0.38       205\n",
      "\n",
      "    accuracy                           0.99     20000\n",
      "   macro avg       0.75      0.65      0.69     20000\n",
      "weighted avg       0.99      0.99      0.99     20000\n",
      "\n",
      "    ROC AUC: 0.8071\n",
      "\n",
      "--- Backtesting Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ks_2samp\n",
    "import string\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- Configuration and Global Variables (consistent with previous steps) ---\n",
    "NUM_CUSTOMERS = 500\n",
    "NUM_TRANSACTIONS = 100000\n",
    "\n",
    "# Country Risk Map (simulated external data)\n",
    "COUNTRY_RISK_MAP = {\n",
    "    'IRAN': 'HIGH', 'NORTH KOREA': 'HIGH', 'SYRIA': 'HIGH', 'CUBA': 'HIGH', 'VENEZUULA': 'HIGH',\n",
    "    'RUSSIA': 'MEDIUM', 'CHINA': 'MEDIUM', 'INDIA': 'LOW', 'USA': 'LOW', 'UK': 'LOW',\n",
    "    'GERMANY': 'LOW', 'FRANCE': 'LOW', 'BRAZIL': 'MEDIUM', 'SOUTH AFRICA': 'MEDIUM',\n",
    "    'NIGERIA': 'MEDIUM', 'AFGHANISTAN': 'HIGH', 'YEMEN': 'HIGH', 'SOMALIA': 'HIGH',\n",
    "    'LEBANON': 'MEDIUM', 'PAKISTAN': 'MEDIUM'\n",
    "}\n",
    "HIGH_RISK_COUNTRIES = [country for country, risk in COUNTRY_RISK_MAP.items() if risk == 'HIGH']\n",
    "LOW_RISK_COUNTRIES = [country for country, risk in COUNTRY_RISK_MAP.items() if risk == 'LOW']\n",
    "\n",
    "# --- Helper Functions (copied for self-containment and consistency) ---\n",
    "\n",
    "def load_common_names_from_excel(filepath, sheet_name):\n",
    "    try:\n",
    "        names_df = pd.read_excel(filepath, sheet_name=sheet_name)\n",
    "        if 'Sanctioned_name' in names_df.columns:\n",
    "            return names_df['Sanctioned_name'].astype(str).tolist()\n",
    "        else:\n",
    "            return _get_default_common_names()\n",
    "    except FileNotFoundError:\n",
    "        return _get_default_common_names()\n",
    "    except Exception as e:\n",
    "        return _get_default_common_names()\n",
    "\n",
    "def _get_default_common_names():\n",
    "    return [\n",
    "        'John Smith', 'Jane Johnson', 'Michael Williams', 'Emily Brown', 'David Jones',\n",
    "        'Sarah Garcia', 'Chris Miller', 'Anna Davis', 'Robert Rodriguez', 'Maria Martinez',\n",
    "        'William Taylor', 'Olivia Wilson', 'James Moore', 'Sophia White', 'Benjamin Green',\n",
    "        'Isabella Hall', 'Lucas King', 'Mia Wright', 'Henry Lopez', 'Charlotte Hill'\n",
    "    ]\n",
    "\n",
    "COMMON_FULL_NAMES = load_common_names_from_excel('Name_list.xlsx', 'Names')\n",
    "if not COMMON_FULL_NAMES:\n",
    "    print(\"Warning: COMMON_FULL_NAMES list is empty after attempting to load from Excel. Using default names.\")\n",
    "    COMMON_FULL_NAMES = _get_default_common_names()\n",
    "\n",
    "def load_or_generate_initial_data(sanctions_csv_path='UK Sanctions List_mean.csv',\n",
    "                                   customer_data_path='customer_data.csv',\n",
    "                                   num_customers=None):\n",
    "    if num_customers is None:\n",
    "        num_customers = NUM_CUSTOMERS\n",
    "\n",
    "    sanctions_df_cleaned = pd.DataFrame()\n",
    "    customer_df = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        raw_sanctions_df = pd.read_csv(sanctions_csv_path, encoding='latin1', header=1)\n",
    "        name_col = 'Name 6'\n",
    "        address_col = 'Address 6'\n",
    "        dob_col = 'DOB 6'\n",
    "        nationality_col = 'Nationality 6'\n",
    "        type_col = 'Type'\n",
    "        id_col = 'ID'\n",
    "\n",
    "        actual_name_col = name_col if name_col in raw_sanctions_df.columns else ('Name' if 'Name' in raw_sanctions_df.columns else None)\n",
    "        actual_address_col = address_col if address_col in raw_sanctions_df.columns else ('Address' if 'Address' in raw_sanctions_df.columns else None)\n",
    "        actual_dob_col = dob_col if dob_col in raw_sanctions_df.columns else ('DOB' if 'DOB' in raw_sanctions_df.columns else None)\n",
    "        actual_nationality_col = nationality_col if nationality_col in raw_sanctions_df.columns else ('Nationality' if 'Nationality' in raw_sanctions_df.columns else None)\n",
    "        actual_type_col = type_col if type_col in raw_sanctions_df.columns else ('Type' if 'Type' in raw_sanctions_df.columns else None)\n",
    "        actual_id_col = id_col if id_col in raw_sanctions_df.columns else ('ID' if 'ID' in raw_sanctions_df.columns else None)\n",
    "\n",
    "        if not actual_name_col:\n",
    "            raise ValueError(\"No name column found\")\n",
    "\n",
    "        sanctions_df_cleaned = raw_sanctions_df.copy()\n",
    "        sanctions_df_cleaned['Sanctioned_Name'] = sanctions_df_cleaned[actual_name_col].astype(str).str.upper().str.strip()\n",
    "        \n",
    "        sanctions_df_cleaned['Sanctioned_Address'] = sanctions_df_cleaned[actual_address_col].astype(str).str.upper().str.strip() if actual_address_col and actual_address_col in sanctions_df_cleaned.columns else np.nan\n",
    "        sanctions_df_cleaned['Sanctioned_DOB'] = pd.to_datetime(sanctions_df_cleaned[actual_dob_col], errors='coerce').dt.strftime('%Y-%m-%d') if actual_dob_col and actual_dob_col in sanctions_df_cleaned.columns else np.nan\n",
    "        sanctions_df_cleaned['Sanctioned_Nationality'] = sanctions_df_cleaned[actual_nationality_col].astype(str).str.upper().str.strip() if actual_nationality_col and actual_nationality_col in sanctions_df_cleaned.columns else np.nan\n",
    "        sanctions_df_cleaned['Sanction_Type'] = sanctions_df_cleaned[actual_type_col].astype(str).str.upper().str.strip() if actual_type_col and actual_type_col in sanctions_df_cleaned.columns else np.nan\n",
    "        sanctions_df_cleaned['Sanctioned_ID'] = sanctions_df_cleaned[actual_id_col].astype(str) if actual_id_col and actual_id_col in sanctions_df_cleaned.columns else [f'S{i:04d}' for i in range(len(sanctions_df_cleaned))]\n",
    "\n",
    "        sanctions_df_cleaned = sanctions_df_cleaned[[\n",
    "            'Sanctioned_ID', 'Sanctioned_Name', 'Sanctioned_Address',\n",
    "            'Sanctioned_DOB', 'Sanctioned_Nationality', 'Sanction_Type'\n",
    "        ]].copy()\n",
    "        sanctions_df_cleaned = sanctions_df_cleaned[\n",
    "            (sanctions_df_cleaned['Sanctioned_Name'] != 'UNKNOWN SANCTIONED NAME') &\n",
    "            (sanctions_df_cleaned['Sanctioned_Name'] != 'NAN') &\n",
    "            (sanctions_df_cleaned['Sanctioned_Name'].str.strip() != '')\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        sanctions_df_cleaned = pd.DataFrame({\n",
    "            'Sanctioned_ID': [f'S{i:04d}' for i in range(1, 101)],\n",
    "            'Sanctioned_Name': [f'SANCTIONED PERSON {i}' for i in range(1, 101)],\n",
    "            'Sanctioned_Address': [f'{i*10} MAIN ST, HIGH RISK COUNTRY' for i in range(1, 101)],\n",
    "            'Sanctioned_DOB': [f'{1950 + i}-01-01' for i in range(100)],\n",
    "            'Sanctioned_Nationality': random.choices(HIGH_RISK_COUNTRIES, k=100),\n",
    "            'Sanction_Type': random.choices(['INDIVIDUAL', 'ENTITY'], k=100)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        sanctions_df_cleaned = pd.DataFrame({\n",
    "            'Sanctioned_ID': [f'S{i:04d}' for i in range(1, 101)],\n",
    "            'Sanctioned_Name': [f'SANCTIONED PERSON {i}' for i in range(1, 101)],\n",
    "            'Sanctioned_Address': [f'{i*10} MAIN ST, HIGH RISK COUNTRY' for i in range(1, 101)],\n",
    "            'Sanctioned_DOB': [f'{1950 + i}-01-01' for i in range(100)],\n",
    "            'Sanctioned_Nationality': random.choices(HIGH_RISK_COUNTRIES, k=100),\n",
    "            'Sanction_Type': random.choices(['INDIVIDUAL', 'ENTITY'], k=100)\n",
    "        })\n",
    "\n",
    "\n",
    "    try:\n",
    "        customer_df = pd.read_csv(customer_data_path)\n",
    "        current_cols_lower = {col.lower(): col for col in customer_df.columns}\n",
    "        \n",
    "        expected_customer_cols_mapping = {\n",
    "            'customer_id': 'Customer_ID',\n",
    "            'customer_name': 'Customer_Name',\n",
    "            'customer_address': 'Customer_Address',\n",
    "            'customer_dob': 'Customer_DOB',\n",
    "            'customer_nationality': 'Customer_Nationality',\n",
    "            'customer_country': 'Customer_Country',\n",
    "            'customer_industry': 'Customer_Industry',\n",
    "            'onboarding_date': 'Onboarding_Date'\n",
    "        }\n",
    "        \n",
    "        rename_dict = {}\n",
    "        for old_col_lower, new_col_proper in expected_customer_cols_mapping.items():\n",
    "            if old_col_lower in current_cols_lower:\n",
    "                rename_dict[current_cols_lower[old_col_lower]] = new_col_proper\n",
    "            elif new_col_proper not in customer_df.columns:\n",
    "                customer_df[new_col_proper] = np.nan\n",
    "\n",
    "        if rename_dict:\n",
    "            customer_df.rename(columns=rename_dict, inplace=True)\n",
    "            \n",
    "        required_customer_cols = ['Customer_ID', 'Customer_Name', 'Customer_Address', 'Customer_DOB', 'Customer_Nationality', 'Customer_Country']\n",
    "        if not all(col in customer_df.columns for col in required_customer_cols):\n",
    "            missing_cols = [col for col in required_customer_cols if col not in customer_df.columns]\n",
    "            customer_df = pd.DataFrame() \n",
    "\n",
    "    except FileNotFoundError:\n",
    "        customers = []\n",
    "        for i in range(1, num_customers + 1):\n",
    "            customer_id = f'CUST{i:05d}'\n",
    "            customer_name = random.choice(COMMON_FULL_NAMES)\n",
    "            customer_address = f\"{random.randint(100, 999)} {random.choice(['Main St', 'Oak Ave', 'Pine Ln'])}\"\n",
    "            customer_dob = (datetime.date(1950, 1, 1) + datetime.timedelta(days=random.randint(0, 365 * 50))).strftime('%Y-%m-%d')\n",
    "            customer_nationality = random.choice(list(COUNTRY_RISK_MAP.keys()))\n",
    "            customer_country = random.choice(list(COUNTRY_RISK_MAP.keys()))\n",
    "            customer_industry = random.choice(['Financial Services', 'Retail', 'Technology', 'Manufacturing', 'Healthcare'])\n",
    "            onboarding_date = (datetime.date(2020, 1, 1) + datetime.timedelta(days=random.randint(0, 365 * 3))).strftime('%Y-%m-%d')\n",
    "            customers.append({\n",
    "                'Customer_ID': customer_id, 'Customer_Name': customer_name, 'Customer_Address': customer_address,\n",
    "                'Customer_DOB': customer_dob, 'Customer_Nationality': customer_nationality, 'Customer_Country': customer_country,\n",
    "                'Customer_Industry': customer_industry, 'Onboarding_Date': onboarding_date\n",
    "            })\n",
    "        customer_df = pd.DataFrame(customers)\n",
    "    except Exception as e:\n",
    "        customers = []\n",
    "        for i in range(1, num_customers + 1):\n",
    "            customer_id = f'CUST{i:05d}'\n",
    "            customer_name = random.choice(COMMON_FULL_NAMES)\n",
    "            customer_address = f\"{random.randint(100, 999)} {random.choice(['Main St', 'Oak Ave', 'Pine Ln'])}\"\n",
    "            customer_dob = (datetime.date(1950, 1, 1) + datetime.timedelta(days=random.randint(0, 365 * 50))).strftime('%Y-%m-%d')\n",
    "            customer_nationality = random.choice(list(COUNTRY_RISK_MAP.keys()))\n",
    "            customer_country = random.choice(list(COUNTRY_RISK_MAP.keys()))\n",
    "            customer_industry = random.choice(['Financial Services', 'Retail', 'Technology', 'Manufacturing', 'Healthcare'])\n",
    "            onboarding_date = (datetime.date(2020, 1, 1) + datetime.timedelta(days=random.randint(0, 365 * 3))).strftime('%Y-%m-%d')\n",
    "            customers.append({\n",
    "                'Customer_ID': customer_id, 'Customer_Name': customer_name, 'Customer_Address': customer_address,\n",
    "                'Customer_DOB': customer_dob, 'Customer_Nationality': customer_nationality, 'Customer_Country': customer_country,\n",
    "                'Customer_Industry': customer_industry, 'Onboarding_Date': onboarding_date\n",
    "            })\n",
    "        customer_df = pd.DataFrame(customers)\n",
    "\n",
    "    return sanctions_df_cleaned, customer_df\n",
    "\n",
    "def calculate_sanctions_features(df):\n",
    "    expected_input_cols = [\n",
    "        'Customer_ID', 'Customer_Name', 'Customer_Address', 'Customer_DOB', 'Customer_Nationality', 'Customer_Country',\n",
    "        'Sanctioned_ID', 'Sanctioned_Name', 'Sanctioned_Address', 'Sanctioned_DOB', 'Sanctioned_Nationality', 'Sanction_Type'\n",
    "    ]\n",
    "    if df.empty:\n",
    "        all_expected_output_cols = expected_input_cols + [\n",
    "            'Customer_Name_Clean', 'Sanctioned_Name_Clean', 'Customer_Address_Clean', 'Sanctioned_Address_Clean',\n",
    "            'Customer_Nationality_Clean', 'Sanctioned_Nationality_Clean', 'Customer_Country_Clean',\n",
    "            'name_fuzz_ratio', 'name_token_sort_ratio', 'name_token_set_ratio', 'name_match_score',\n",
    "            'address_match_score', 'dob_match', 'nationality_match', 'customer_country_risk_score',\n",
    "            'sanction_type_severity_score', 'name_country_interaction', 'name_dob_interaction'\n",
    "        ]\n",
    "        if 'is_sanction_match' in df.columns:\n",
    "            all_expected_output_cols.append('is_sanction_match')\n",
    "        return pd.DataFrame(columns=all_expected_output_cols)\n",
    "\n",
    "    for col in expected_input_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    df['Customer_Name_Clean'] = df['Customer_Name'].astype(str).str.upper().str.strip()\n",
    "    df['Sanctioned_Name_Clean'] = df['Sanctioned_Name'].astype(str).str.upper().str.strip()\n",
    "    df['Customer_Address_Clean'] = df['Customer_Address'].astype(str).str.upper().str.strip()\n",
    "    df['Sanctioned_Address_Clean'] = df['Sanctioned_Address'].astype(str).str.upper().str.strip()\n",
    "    df['Customer_Nationality_Clean'] = df['Customer_Nationality'].astype(str).str.upper().str.strip()\n",
    "    df['Sanctioned_Nationality_Clean'] = df['Sanctioned_Nationality'].astype(str).str.upper().str.strip()\n",
    "    df['Customer_Country_Clean'] = df['Customer_Country'].astype(str).str.upper().str.strip()\n",
    "\n",
    "    df['name_fuzz_ratio'] = df.apply(lambda row: fuzz.ratio(row['Customer_Name_Clean'], row['Sanctioned_Name_Clean']), axis=1)\n",
    "    df['name_token_sort_ratio'] = df.apply(lambda row: fuzz.token_sort_ratio(row['Customer_Name_Clean'], row['Sanctioned_Name_Clean']), axis=1)\n",
    "    df['name_token_set_ratio'] = df.apply(lambda row: fuzz.token_set_ratio(row['Customer_Name_Clean'], row['Sanctioned_Name_Clean']), axis=1)\n",
    "    df['name_match_score'] = df[['name_fuzz_ratio', 'name_token_sort_ratio', 'name_token_set_ratio']].max(axis=1)\n",
    "    df['address_match_score'] = df.apply(lambda row: fuzz.token_set_ratio(row['Customer_Address_Clean'], row['Sanctioned_Address_Clean']), axis=1)\n",
    "    df['dob_match'] = df.apply(lambda row: 1 if (pd.notna(row['Customer_DOB']) and pd.notna(row['Sanctioned_DOB']) and str(row['Customer_DOB']) == str(row['Sanctioned_DOB'])) else 0, axis=1)\n",
    "    df['nationality_match'] = df.apply(lambda row: 1 if (pd.notna(row['Customer_Nationality_Clean']) and pd.notna(row['Sanctioned_Nationality_Clean']) and row['Customer_Nationality_Clean'] == row['Sanctioned_Nationality_Clean']) else 0, axis=1)\n",
    "\n",
    "    df['customer_country_risk_score'] = df['Customer_Country_Clean'].map(\n",
    "        {k: (10 if v == 'HIGH' else 5 if v == 'MEDIUM' else 1) for k, v in COUNTRY_RISK_MAP.items()}\n",
    "    ).fillna(0)\n",
    "\n",
    "    sanction_type_severity = {\n",
    "        'INDIVIDUAL': 10, 'ENTITY': 8, 'VESSEL': 5, 'AIRCRAFT': 5, 'NAN': 0, 'UNKNOWN': 0\n",
    "    }\n",
    "    df['sanction_type_severity_score'] = df['Sanction_Type'].map(sanction_type_severity).fillna(0)\n",
    "    df['name_country_interaction'] = df['name_match_score'] * df['customer_country_risk_score']\n",
    "    df['name_dob_interaction'] = df['name_match_score'] * df['dob_match']\n",
    "    return df\n",
    "\n",
    "def generate_transaction_data(customer_df, num_transactions=None):\n",
    "    if num_transactions is None:\n",
    "        num_transactions = NUM_TRANSACTIONS\n",
    "\n",
    "    transactions = []\n",
    "    transaction_types = ['DEPOSIT', 'WITHDRAWAL', 'TRANSFER_IN', 'TRANSFER_OUT', 'PAYMENT']\n",
    "    currencies = ['USD', 'EUR', 'GBP', 'JPY']\n",
    "    \n",
    "    if customer_df.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            'Transaction_ID', 'Customer_ID', 'Transaction_Date', 'Transaction_Type',\n",
    "            'Amount', 'Currency', 'Sender_ID', 'Receiver_ID', 'Sender_Country',\n",
    "            'Receiver_Country', 'Is_Suspicious_Label'\n",
    "        ])\n",
    "\n",
    "    customer_ids = customer_df['Customer_ID'].tolist()\n",
    "    customer_countries = customer_df.set_index('Customer_ID')['Customer_Country'].to_dict()\n",
    "\n",
    "    for i in range(num_transactions):\n",
    "        trans_id = f'TRANS{i:07d}'\n",
    "        customer_id = random.choice(customer_ids)\n",
    "        trans_date = (datetime.date(2023, 1, 1) + datetime.timedelta(days=random.randint(0, 364))).strftime('%Y-%m-%d')\n",
    "        trans_type = random.choice(transaction_types)\n",
    "        amount = round(random.uniform(10, 10000), 2)\n",
    "        currency = random.choice(currencies)\n",
    "        \n",
    "        sender_id = customer_id\n",
    "        receiver_id = random.choice(customer_ids)\n",
    "        while sender_id == receiver_id and trans_type in ['TRANSFER_IN', 'TRANSFER_OUT']:\n",
    "            receiver_id = random.choice(customer_ids)\n",
    "\n",
    "        sender_country = customer_countries.get(sender_id, random.choice(list(COUNTRY_RISK_MAP.keys())))\n",
    "        receiver_country = customer_countries.get(receiver_id, random.choice(list(COUNTRY_RISK_MAP.keys())))\n",
    "\n",
    "        is_suspicious = 0\n",
    "        if random.random() < 0.01:\n",
    "            is_suspicious = 1\n",
    "            susp_type = random.choice(['large_amount', 'high_risk_country', 'structuring'])\n",
    "\n",
    "            if susp_type == 'large_amount':\n",
    "                amount = round(random.uniform(50000, 1000000), 2)\n",
    "            elif susp_type == 'high_risk_country':\n",
    "                if trans_type in ['TRANSFER_OUT', 'PAYMENT']:\n",
    "                    receiver_country = random.choice(HIGH_RISK_COUNTRIES)\n",
    "                else:\n",
    "                    sender_country = random.choice(HIGH_RISK_COUNTRIES)\n",
    "                amount = round(random.uniform(5000, 50000), 2)\n",
    "            elif susp_type == 'structuring':\n",
    "                amount = round(random.uniform(8000, 9900), 2)\n",
    "                trans_type = random.choice(['DEPOSIT', 'WITHDRAWAL'])\n",
    "\n",
    "        transactions.append({\n",
    "            'Transaction_ID': trans_id,\n",
    "            'Customer_ID': customer_id,\n",
    "            'Transaction_Date': trans_date,\n",
    "            'Transaction_Type': trans_type,\n",
    "            'Amount': amount,\n",
    "            'Currency': currency,\n",
    "            'Sender_ID': sender_id,\n",
    "            'Receiver_ID': receiver_id,\n",
    "            'Sender_Country': sender_country,\n",
    "            'Receiver_Country': receiver_country,\n",
    "            'Is_Suspicious_Label': is_suspicious\n",
    "        })\n",
    "\n",
    "    transaction_df = pd.DataFrame(transactions)\n",
    "    transaction_df['Transaction_Date'] = pd.to_datetime(transaction_df['Transaction_Date'])\n",
    "    \n",
    "    return transaction_df\n",
    "\n",
    "def feature_engineer_transactions(transactions_df, customer_df):\n",
    "    required_cols = ['Customer_ID', 'Transaction_Date', 'Amount', 'Transaction_Type', 'Sender_Country', 'Receiver_Country']\n",
    "    for col in required_cols:\n",
    "        if col not in transactions_df.columns:\n",
    "            transactions_df[col] = np.nan\n",
    "\n",
    "    transactions_df['Transaction_Date'] = pd.to_datetime(transactions_df['Transaction_Date'], errors='coerce')\n",
    "    transactions_df.dropna(subset=['Transaction_Date'], inplace=True)\n",
    "\n",
    "    transactions_df['Amount_USD'] = transactions_df['Amount']\n",
    "    transactions_df['Transaction_Hour'] = transactions_df['Transaction_Date'].dt.hour\n",
    "    transactions_df['Transaction_DayOfWeek'] = transactions_df['Transaction_Date'].dt.dayofweek\n",
    "\n",
    "    transactions_df['Sender_Country_Risk_Score'] = transactions_df['Sender_Country'].map(\n",
    "        {k: (10 if v == 'HIGH' else 5 if v == 'MEDIUM' else 1) for k, v in COUNTRY_RISK_MAP.items()}\n",
    "    ).fillna(0)\n",
    "    transactions_df['Receiver_Country_Risk_Score'] = transactions_df['Receiver_Country'].map(\n",
    "        {k: (10 if v == 'HIGH' else 5 if v == 'MEDIUM' else 1) for k, v in COUNTRY_RISK_MAP.items()}\n",
    "    ).fillna(0)\n",
    "    transactions_df['Geographic_Risk_Score'] = transactions_df[['Sender_Country_Risk_Score', 'Receiver_Country_Risk_Score']].max(axis=1)\n",
    "\n",
    "    transactions_df = pd.get_dummies(transactions_df, columns=['Transaction_Type'], prefix='TxType', dummy_na=False)\n",
    "\n",
    "    customer_agg_features = transactions_df.groupby('Customer_ID').agg(\n",
    "        Total_Amount=('Amount_USD', 'sum'),\n",
    "        Avg_Amount=('Amount_USD', 'mean'),\n",
    "        Num_Transactions=('Transaction_ID', 'count'),\n",
    "        Max_Amount=('Amount_USD', 'max'),\n",
    "        Min_Amount=('Amount_USD', 'min'),\n",
    "        Unique_Counterparties=('Receiver_ID', lambda x: x.nunique()),\n",
    "    ).reset_index()\n",
    "\n",
    "    transactions_df_features = pd.merge(transactions_df, customer_agg_features, on='Customer_ID', how='left')\n",
    "    transactions_df_features['Amount_Geo_Risk_Interaction'] = transactions_df_features['Amount_USD'] * transactions_df_features['Geographic_Risk_Score']\n",
    "\n",
    "    model_features = [\n",
    "        'Amount_USD', 'Transaction_Hour', 'Transaction_DayOfWeek',\n",
    "        'Sender_Country_Risk_Score', 'Receiver_Country_Risk_Score', 'Geographic_Risk_Score',\n",
    "        'Total_Amount', 'Avg_Amount', 'Num_Transactions', 'Max_Amount', 'Min_Amount', 'Unique_Counterparties',\n",
    "        'Amount_Geo_Risk_Interaction'\n",
    "    ]\n",
    "    \n",
    "    for col in transactions_df_features.columns:\n",
    "        if col.startswith('TxType_'):\n",
    "            model_features.append(col)\n",
    "    \n",
    "    for feature in model_features:\n",
    "        if feature not in transactions_df_features.columns:\n",
    "            transactions_df_features[feature] = 0\n",
    "\n",
    "    return transactions_df_features, model_features\n",
    "\n",
    "def train_aml_model(X_train_scaled, contamination_rate=0.01):\n",
    "    model = IsolationForest(random_state=42, contamination=contamination_rate, n_estimators=200, max_features=1.0)\n",
    "    model.fit(X_train_scaled)\n",
    "    return model\n",
    "\n",
    "def train_sanctions_model(X_train, y_train, model_type='GradientBoosting'):\n",
    "    if model_type == 'LogisticRegression':\n",
    "        model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', max_iter=1000)\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "    elif model_type == 'GradientBoosting':\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'LogisticRegression' or 'GradientBoosting'\")\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def generate_and_label_training_data(customer_df, sanctions_df_cleaned, num_samples=20000, start_date_offset_years=0):\n",
    "    expected_training_df_cols = [\n",
    "        'Customer_ID', 'Customer_Name', 'Customer_Address', 'Customer_DOB', 'Customer_Nationality', 'Customer_Country',\n",
    "        'Sanctioned_ID', 'Sanctioned_Name', 'Sanctioned_Address', 'Sanctioned_DOB', 'Sanctioned_Nationality', 'Sanction_Type',\n",
    "        'is_sanction_match'\n",
    "    ]\n",
    "    if customer_df.empty or sanctions_df_cleaned.empty:\n",
    "        return pd.DataFrame(columns=expected_training_df_cols)\n",
    "    \n",
    "    if len(customer_df) < 1 or len(sanctions_df_cleaned) < 1:\n",
    "        return pd.DataFrame(columns=expected_training_df_cols)\n",
    "\n",
    "    training_samples = []\n",
    "    num_true_positives = int(num_samples * 0.05)\n",
    "    for _ in range(num_true_positives):\n",
    "        sanctioned_entity = sanctions_df_cleaned.sample(1).iloc[0]\n",
    "        customer_entity = customer_df.sample(1).iloc[0]\n",
    "        cust_name_tp = sanctioned_entity['Sanctioned_Name']\n",
    "        if len(cust_name_tp) > 3:\n",
    "            idx = random.randint(0, len(cust_name_tp) - 1)\n",
    "            cust_name_tp = cust_name_tp[:idx] + random.choice(string.ascii_uppercase) + cust_name_tp[idx+1:]\n",
    "        cust_address_tp = sanctioned_entity['Sanctioned_Address']\n",
    "        if len(cust_address_tp) > 5:\n",
    "            idx = random.randint(0, len(cust_address_tp) - 1)\n",
    "            cust_address_tp = cust_address_tp[:idx] + random.choice(string.ascii_uppercase) + cust_address_tp[idx+1:]\n",
    "        \n",
    "        # Adjust DOB for backtesting if needed, e.g., shift earlier\n",
    "        original_dob = pd.to_datetime(sanctioned_entity['Sanctioned_DOB'], errors='coerce')\n",
    "        if pd.notna(original_dob):\n",
    "            cust_dob_tp = (original_dob - pd.DateOffset(years=start_date_offset_years)).strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            cust_dob_tp = (datetime.date(1950, 1, 1) - datetime.timedelta(days=365*start_date_offset_years)).strftime('%Y-%m-%d')\n",
    "\n",
    "        cust_nationality_tp = sanctioned_entity['Sanctioned_Nationality']\n",
    "        cust_country_tp = sanctioned_entity['Sanctioned_Nationality']\n",
    "        training_samples.append({\n",
    "            'Customer_ID': customer_entity['Customer_ID'], 'Customer_Name': cust_name_tp, 'Customer_Address': cust_address_tp,\n",
    "            'Customer_DOB': cust_dob_tp, 'Customer_Nationality': cust_nationality_tp, 'Customer_Country': cust_country_tp,\n",
    "            'Sanctioned_ID': sanctioned_entity['Sanctioned_ID'], 'Sanctioned_Name': sanctioned_entity['Sanctioned_Name'],\n",
    "            'Sanctioned_Address': sanctioned_entity['Sanctioned_Address'], 'Sanctioned_DOB': sanctioned_entity['Sanctioned_DOB'],\n",
    "            'Sanctioned_Nationality': sanctioned_entity['Sanctioned_Nationality'], 'Sanction_Type': sanctioned_entity['Sanction_Type'],\n",
    "            'is_sanction_match': 1\n",
    "        })\n",
    "\n",
    "    num_true_negatives = num_samples - num_true_positives\n",
    "    for _ in range(num_true_negatives):\n",
    "        sanctioned_entity = sanctions_df_cleaned.sample(1).iloc[0]\n",
    "        customer_entity = customer_df.sample(1).iloc[0]\n",
    "        cust_name_tn = random.choice(COMMON_FULL_NAMES)\n",
    "        cust_address_tn = f\"{random.randint(1000, 9999)} {random.choice(['Road', 'Lane', 'Square'])}\"\n",
    "        \n",
    "        cust_dob_tn = (datetime.date(1940, 1, 1) + datetime.timedelta(days=random.randint(0, 365 * 60)) - datetime.timedelta(days=365*start_date_offset_years)).strftime('%Y-%m-%d')\n",
    "        cust_nationality_tn = random.choice(list(COUNTRY_RISK_MAP.keys()))\n",
    "        cust_country_tn = random.choice(list(COUNTRY_RISK_MAP.keys()))\n",
    "        training_samples.append({\n",
    "            'Customer_ID': customer_entity['Customer_ID'], 'Customer_Name': cust_name_tn, 'Customer_Address': cust_address_tn,\n",
    "            'Customer_DOB': cust_dob_tn, 'Customer_Nationality': cust_nationality_tn, 'Customer_Country': cust_country_tn,\n",
    "            'Sanctioned_ID': sanctioned_entity['Sanctioned_ID'], 'Sanctioned_Name': sanctioned_entity['Sanctioned_Name'],\n",
    "            'Sanctioned_Address': sanctioned_entity['Sanctioned_Address'], 'Sanctioned_DOB': sanctioned_entity['Sanctioned_DOB'],\n",
    "            'Sanctioned_Nationality': sanctioned_entity['Sanctioned_Nationality'], 'Sanction_Type': sanctioned_entity['Sanction_Type'],\n",
    "            'is_sanction_match': 0\n",
    "        })\n",
    "    \n",
    "    training_df = pd.DataFrame(training_samples, columns=expected_training_df_cols)\n",
    "    required_cols_for_features_input = [\n",
    "        'Customer_ID', 'Customer_Name', 'Customer_Address', 'Customer_DOB', 'Customer_Nationality', 'Customer_Country',\n",
    "        'Sanctioned_ID', 'Sanctioned_Name', 'Sanctioned_Address', 'Sanctioned_DOB', 'Sanctioned_Nationality', 'Sanction_Type'\n",
    "    ]\n",
    "    for col in required_cols_for_features_input:\n",
    "        if col not in training_df.columns:\n",
    "            training_df[col] = np.nan\n",
    "\n",
    "    training_df_features = calculate_sanctions_features(training_df.copy())\n",
    "    \n",
    "    if 'is_sanction_match' not in training_df_features.columns:\n",
    "        if 'is_sanction_match' in training_df.columns:\n",
    "            training_df_features['is_sanction_match'] = training_df['is_sanction_match']\n",
    "        else:\n",
    "            training_df_features['is_sanction_match'] = 0\n",
    "\n",
    "    feature_cols = [\n",
    "        'name_match_score', 'address_match_score', 'dob_match',\n",
    "        'nationality_match', 'customer_country_risk_score', 'sanction_type_severity_score',\n",
    "        'name_country_interaction', 'name_dob_interaction'\n",
    "    ]\n",
    "    for col in feature_cols:\n",
    "        if col not in training_df_features.columns:\n",
    "            training_df_features[col] = 0\n",
    "\n",
    "    training_df_features[feature_cols] = training_df_features[feature_cols].fillna(0)\n",
    "    return training_df_features\n",
    "\n",
    "def evaluate_model_performance(model_type, model, X_test, y_test_true, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluates and prints performance metrics.\n",
    "    \"\"\"\n",
    "    if model_type == 'Sanctions':\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        report = classification_report(y_test_true, y_pred, output_dict=True)\n",
    "        roc_auc = roc_auc_score(y_test_true, y_proba)\n",
    "\n",
    "        print(f\"  {model_name} Performance:\")\n",
    "        print(f\"    Accuracy: {report['accuracy']:.4f}\")\n",
    "        print(f\"    Precision (Sanctioned): {report['1']['precision']:.4f}\")\n",
    "        print(f\"    Recall (Sanctioned): {report['1']['recall']:.4f}\")\n",
    "        print(f\"    F1-Score (Sanctioned): {report['1']['f1-score']:.4f}\")\n",
    "        print(f\"    ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    elif model_type == 'AML':\n",
    "        y_pred_aml = np.where(model.predict(X_test) == -1, 1, 0) # -1 for outlier -> 1 for suspicious\n",
    "        print(f\"  {model_name} Performance:\")\n",
    "        print(classification_report(y_test_true, y_pred_aml, target_names=['Normal', 'Suspicious']))\n",
    "        try:\n",
    "            anomaly_scores = model.decision_function(X_test)\n",
    "            roc_auc = roc_auc_score(y_test_true, -anomaly_scores)\n",
    "            print(f\"    ROC AUC: {roc_auc:.4f}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"    Could not compute ROC AUC: {e}. (Likely only one class present in true labels)\")\n",
    "    else:\n",
    "        print(\"Invalid model_type for evaluation.\")\n",
    "\n",
    "# --- Main Execution Flow for Backtesting ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Backtesting Analysis ---\")\n",
    "\n",
    "    # 1. Load Initial Data and Trained Models\n",
    "    # We load initial data to use for generating *new* historical data\n",
    "    sanctions_df, customer_df = load_or_generate_initial_data()\n",
    "    if customer_df.empty or sanctions_df.empty:\n",
    "        print(\"FATAL: Customer or sanctions data is empty. Cannot proceed with backtesting. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    sanctions_model_filename = 'sanctions_screening_gb_model.joblib'\n",
    "    aml_model_filename = 'aml_isolation_forest_model.joblib'\n",
    "    aml_scaler_filename = 'aml_scaler.joblib'\n",
    "\n",
    "    sanctions_model = None\n",
    "    aml_model = None\n",
    "    aml_scaler = None\n",
    "\n",
    "    try:\n",
    "        sanctions_model = joblib.load(sanctions_model_filename)\n",
    "        print(f\"Loaded Sanctions Screening Model from '{sanctions_model_filename}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Sanctions model '{sanctions_model_filename}' not found. Skipping sanctions backtesting.\")\n",
    "\n",
    "    try:\n",
    "        aml_model = joblib.load(aml_model_filename)\n",
    "        aml_scaler = joblib.load(aml_scaler_filename)\n",
    "        print(f\"Loaded AML Isolation Forest Model and Scaler from '{aml_model_filename}' and '{aml_scaler_filename}'.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: AML model or scaler not found. Skipping AML backtesting.\")\n",
    "\n",
    "    # Define feature lists (ensure consistency with training)\n",
    "    sanctions_ml_features = [\n",
    "        'name_match_score', 'address_match_score', 'dob_match',\n",
    "        'nationality_match', 'customer_country_risk_score', 'sanction_type_severity_score',\n",
    "        'name_country_interaction', 'name_dob_interaction'\n",
    "    ]\n",
    "    \n",
    "    # For AML features, we need to generate some data to get the full list of TxType_ columns\n",
    "    temp_aml_data, temp_aml_features_list = feature_engineer_transactions(generate_transaction_data(customer_df.head(10)), customer_df.head(10))\n",
    "    aml_features_list = temp_aml_features_list # This will include all possible TxType_ columns\n",
    "\n",
    "    # --- 2. Backtest Sanctions Screening Model ---\n",
    "    if sanctions_model:\n",
    "        print(\"\\n--- Running Sanctions Model Backtest ---\")\n",
    "        # Simulate historical sanctions test data (e.g., from a period 1 year ago)\n",
    "        # Using a distinct set of samples, potentially with slight historical variations in labels if needed\n",
    "        historical_sanctions_test_data = generate_and_label_training_data(customer_df.copy(), sanctions_df.copy(), num_samples=5000, start_date_offset_years=1)\n",
    "        \n",
    "        if not historical_sanctions_test_data.empty:\n",
    "            # Ensure all expected features are present, fill with 0 if not\n",
    "            for feature in sanctions_ml_features:\n",
    "                if feature not in historical_sanctions_test_data.columns:\n",
    "                    historical_sanctions_test_data[feature] = 0\n",
    "\n",
    "            X_historical_sanctions = historical_sanctions_test_data[sanctions_ml_features].fillna(0)\n",
    "            y_historical_sanctions = historical_sanctions_test_data['is_sanction_match']\n",
    "\n",
    "            if len(y_historical_sanctions.unique()) < 2:\n",
    "                print(\"Warning: Historical sanctions data has only one class in true labels. Cannot compute full classification report.\")\n",
    "            else:\n",
    "                evaluate_model_performance('Sanctions', sanctions_model, X_historical_sanctions, y_historical_sanctions, \"Historical Sanctions Model\")\n",
    "        else:\n",
    "            print(\"Skipping Sanctions Model Backtest due to empty historical data.\")\n",
    "    else:\n",
    "        print(\"Sanctions Model not loaded, skipping backtest.\")\n",
    "\n",
    "    # --- 3. Backtest AML Transaction Monitoring Model ---\n",
    "    if aml_model and aml_scaler:\n",
    "        print(\"\\n--- Running AML Model Backtest ---\")\n",
    "        # Simulate historical AML transaction data\n",
    "        historical_aml_raw = generate_transaction_data(customer_df.copy(), num_transactions=20000)\n",
    "        \n",
    "        if not historical_aml_raw.empty:\n",
    "            historical_aml_features, _ = feature_engineer_transactions(historical_aml_raw.copy(), customer_df.copy())\n",
    "            \n",
    "            # Ensure all expected features are present in historical_aml_features\n",
    "            for feature in aml_features_list:\n",
    "                if feature not in historical_aml_features.columns:\n",
    "                    historical_aml_features[feature] = 0\n",
    "\n",
    "            X_historical_aml = historical_aml_features[aml_features_list].fillna(0)\n",
    "            y_historical_aml_labels = historical_aml_features['Is_Suspicious_Label']\n",
    "\n",
    "            # Scale historical data using the *same scaler* that was fitted on the training data\n",
    "            X_historical_aml_scaled = aml_scaler.transform(X_historical_aml)\n",
    "            \n",
    "            evaluate_model_performance('AML', aml_model, X_historical_aml_scaled, y_historical_aml_labels, \"Historical AML Model\")\n",
    "        else:\n",
    "            print(\"Skipping AML Model Backtest due to empty historical data.\")\n",
    "    else:\n",
    "        print(\"AML Model or Scaler not loaded, skipping backtest.\")\n",
    "\n",
    "    print(\"\\n--- Backtesting Analysis Complete ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
